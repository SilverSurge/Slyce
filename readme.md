# ğŸ§  Slyce: Dynamic Slice-Based PyTorch Models

**Slyce** is a dynamic memory-aware architecture for PyTorch that lets you load, unload, and cache model components ("slices") across GPU, CPU, and diskâ€”enabling large models to run efficiently even on limited hardware.

## âœ¨ Features

- **Slice-Based Execution**: Only one slice is active at a time to conserve memory.
- **GPU Strategy Options**:
  - `'none'`: Run on CPU only.
  - `'active_only'`: Only keep the currently active slice on GPU.
  - `'limit_gpu'`: Keep as many slices as fit within a GPU memory budget.
- **Multi-Level Caching**: Automatically moves slices from GPU â†’ CPU â†’ disk.
- **Memory Tracking**: Uses PyTorch and `psutil` to monitor GPU/CPU memory usage.
- **Smart Logging**: Logs slice activation, memory usage, and caching behavior to both file and console.
- **Modular Design**: Each slice is built from a constructor and config for full flexibility.


## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/slyce.git
cd slyce
pip install -r requirements.txt
```

> âš ï¸ Make sure your environment supports PyTorch and `psutil`.

---

## ğŸ§ª Quick Start Example (SUBJECT TO CHANGES)

```python
from slyce import SlyceModel

# Define your slices
slices = [
    {"name": "encoder", "constructor": Encoder, "config": encoder_cfg},
    {"name": "decoder", "constructor": Decoder, "config": decoder_cfg}
]

# Initialize Slyce model
model = SlyceModel(
    slices=slices,
    gpu_strategy="limit_gpu",
    max_gpu_mem_mb=6000,
    disk_cache_path="./slyce_cache"
)

# Use the encoder slice
output = model.forward(input_tensor, active_slice="encoder")

# Switch to decoder
model.set_active_slice("decoder")
output = model.forward(input_tensor)
```

---

## ğŸ“‚ Project Structure (STILL IN Development Stages)

```
slyce/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ core.py         # Core logic of SlyceModel
â”œâ”€â”€ memory.py       # GPU/CPU memory tracking
â”œâ”€â”€ cache.py        # Disk and CPU caching
â”œâ”€â”€ utils.py        # Logging and file utilities
```

---

## ğŸ§  Design Goals

- Efficient use of GPU memory without sacrificing modularity.
- Run large models on consumer hardware.
- Minimize memory spikes and avoid out-of-memory errors.
- Keep things simple before optimizing for complexity.

---

## ğŸ”­ Roadmap

- [x] GPU/CPU memory tracking
- [x] Multi-level caching (GPU â†’ CPU â†’ Disk)
- [x] Logging to file and console
- [ ] SliceManager with usage-aware eviction
- [ ] Asynchronous slice loading/offloading
- [ ] Automatic slice scheduling
- [ ] Web dashboard / CLI for monitoring slices
- [ ] Pre-built models and benchmarks
---

## ğŸ“œ License

MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ‘‹ Author

Made with â¤ï¸ by [Your Name](https://github.com/yourusername)

```

---

Let me know if youâ€™d like a `.md` file download or if you want to plug in real model examples like `nn.TransformerEncoderLayer` etc.